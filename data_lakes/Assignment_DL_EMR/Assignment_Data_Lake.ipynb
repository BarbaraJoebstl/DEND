{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "everyday-second",
   "metadata": {},
   "source": [
    "# Data Lake\n",
    "##Assignment 4 - Data Engineer Nanodegree\n",
    "\n",
    "\n",
    "### Project Datasets\n",
    "taken from http://millionsongdataset.com/ \\\n",
    "Song data: `s3://udacity-dend/song_data`\\\n",
    "Log data: `s3://udacity-dend/log_data`\\\n",
    "Log data json path: `s3://udacity-dend/log_json_path.json`\\\n",
    "Sample Song File:\n",
    "`{\n",
    "    \"num_songs\": 1,\n",
    "    \"artist_id\": \"ARJIE2Y1187B994AB7\",\n",
    "    \"artist_latitude\": null,\n",
    "    \"artist_longitude\": null, \n",
    "    \"artist_location\": \"\", \n",
    "    \"artist_name\": \"Line Renaud\",\n",
    "    \"song_id\": \"SOUPIRU12A6D4FA1E1\", \n",
    "    \"title\": \"Der Kleine Dompfaff\",\n",
    "    \"duration\": 152.92036,\n",
    "    \"year\": 0\n",
    "}`\n",
    "\n",
    "For testing reasons we have a folder with some sample data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-practitioner",
   "metadata": {},
   "source": [
    "### Step 0 - Run Spark locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stunning-headset",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "coral-receiver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure openJDK is installed in your conda env\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"local data lake test\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "muslim-karma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'local-1613749961417'),\n",
       " ('spark.driver.host', 'mbio-mbpro-417.fritz.box'),\n",
       " ('spark.driver.port', '60150'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.name', 'local data lake test'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "useful-picture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://mbio-mbpro-417.fritz.box:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>local data lake test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9a5a616110>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-circumstances",
   "metadata": {},
   "source": [
    " ### Step 1 data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-discipline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip if not done\n",
    "# import zipfile\n",
    "# with zipfile.ZipFile(\"data/song_data.zip\",\"r\") as zip_ref:\n",
    "#    zip_ref.extractall(\"data/song_data\")\n",
    "# with zipfile.ZipFile(\"data/log_data.zip\",\"r\") as zip_ref:\n",
    "#    zip_ref.extractall(\"data/log_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "listed-factor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"data/song_data/*/*/*/*.json\"\n",
    "song_data = spark.read.json(path)\n",
    "song_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "intelligent-organic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, artist_id: string, artist_latitude: string, artist_location: string, artist_longitude: string, artist_name: string, duration: string, num_songs: string, song_id: string, title: string, year: string]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "unlike-category",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "vulnerable-throat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"data/log-data/*.json\"\n",
    "log_data = spark.read.json(path)\n",
    "log_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "played-serial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, artist: string, auth: string, firstName: string, gender: string, itemInSession: string, lastName: string, length: string, level: string, location: string, method: string, page: string, registration: string, sessionId: string, song: string, status: string, ts: string, userAgent: string, userId: string]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dutch-arrest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8056"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-north",
   "metadata": {},
   "source": [
    "### Step 2 - Create Tables\n",
    "#### Fact Table\n",
    "*songplays* - records in log data associated with song plays i.e. records with page NextSong\n",
    "songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent\n",
    "#### Dimension Tables\n",
    "*users* - users in the app\n",
    "user_id, first_name, last_name, gender, level\n",
    "\n",
    "*songs* - songs in music database\n",
    "song_id, title, artist_id, year, duration\n",
    "\n",
    "*artists* - artists in music database\n",
    "artist_id, name, location, lattitude, longitude\n",
    "\n",
    "*time* - timestamps of records in songplays broken down into specific units\n",
    "start_time, hour, day, week, month, year, weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "secure-technician",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ts: long (nullable = true)\n",
      " |-- start_time: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# create time table\n",
    "time_data = log_data.withColumn('start_time', F.from_unixtime(F.col('ts')/1000))\n",
    "time_data = time.select('ts', 'start_time') \\\n",
    "        .withColumn('year', F.year('start_time')) \\\n",
    "        .withColumn('month', F.month('start_time')) \\\n",
    "        .withColumn('week', F.weekofyear('start_time')) \\\n",
    "        .withColumn('weekday', F.dayofweek('start_time')) \\\n",
    "        .withColumn('day', F.dayofyear('start_time')) \\\n",
    "        .withColumn('hour', F.hour('start_time')) \\\n",
    "\n",
    "time_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "gentle-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temporary views of the table on memory - not persistent\n",
    "\n",
    "song_data.createOrReplaceTempView('song_data')\n",
    "log_data.createOrReplaceTempView('log_data')\n",
    "time_data.createOrReplaceTempView('time_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "numerical-narrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# songplays table\n",
    "# songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent\n",
    "songplays = spark.sql(\"\"\"\n",
    "SELECT DISTINCT\n",
    "    l.ts as songplay_id,\n",
    "    l.ts as start_time,\n",
    "    l.userId as user_id,\n",
    "    l.level as level,\n",
    "    s.song_id as song_id,\n",
    "    s.artist_id as artist_id,\n",
    "    l.sessionId as session_id,\n",
    "    l.location as location,\n",
    "    l.userAgent as user_agent\n",
    "FROM song_data s\n",
    "JOIN log_data l\n",
    "    ON s.artist_name = l.artist\n",
    "    AND s.title = l.song\n",
    "    ANd s.duration = l.length\n",
    "JOIN time_data t\n",
    "    ON t.ts = l.ts    \n",
    "\"\"\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "united-geometry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songplays.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "backed-banking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------\n",
      " songplay_id | 1542837407796        \n",
      " start_time  | 1542837407796        \n",
      " user_id     | 15                   \n",
      " level       | paid                 \n",
      " song_id     | SOZCTXZ12AB0182364   \n",
      " artist_id   | AR5KOSW1187FB35FF4   \n",
      " session_id  | 818                  \n",
      " location    | Chicago-Napervill... \n",
      " user_agent  | \"Mozilla/5.0 (X11... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "songplays.show(1, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "lonely-allergy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# users: table - user_id, first_name, last_name, gender, level\n",
    "\n",
    "# if renaming desired\n",
    "#users = spark.sql(\"\"\"\n",
    "#SELECT DISTINCT\n",
    "#    l.userId as user_id,\n",
    "#    l.firstName as first_name,\n",
    "#    l.lastName as last_name,\n",
    "#    l.gender as gender,\n",
    "#    l.level as level\n",
    "#FROM log_data l\n",
    "#\"\"\")\n",
    "\n",
    "users = log_data.select('userId', 'firstName', 'lastName', 'gender', 'level').dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "restricted-dutch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "rough-creek",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------\n",
      " userId    | 57        \n",
      " firstName | Katherine \n",
      " lastName  | Gay       \n",
      " gender    | F         \n",
      " level     | free      \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users.show(1, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "coordinate-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "#songs: song_id, title, artist_id, year, duration\n",
    "songs = song_data.select('song_id', 'title', 'artist_id', 'year', 'duration').dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "million-vertex",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "linear-basis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------\n",
      " song_id   | SOGOSOV12AF72A285E \n",
      " title     | ¿Dónde va Chichi?  \n",
      " artist_id | ARGUVEV1187B98BA17 \n",
      " year      | 1997               \n",
      " duration  | 313.12934          \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songs.show(1, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "leading-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# artists: artist_id, name, location, lattitude, longitude\n",
    "artists = song_data.select('artist_id', 'artist_name', 'title', 'artist_location', 'artist_latitude', 'artist_longitude').dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "earlier-underground",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artists.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "nasty-second",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------\n",
      " artist_id        | ARWB3G61187FB49404 \n",
      " artist_name      | Steve Morse        \n",
      " title            | Prognosis          \n",
      " artist_location  | Hamilton, Ohio     \n",
      " artist_latitude  | null               \n",
      " artist_longitude | null               \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artists.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-reputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 3 - Export Data to S3\n",
    "artists.write.parquet('f{output_data}/artists_table', mode='overwrite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
